{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (of Assignment 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Find all meanings and the noun meaning\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure you have downloaded the necessary data files\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all meanings of a word\n",
    "def get_all_meanings(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    print(f\"Definitions for '{word}':\")\n",
    "    for i, synset in enumerate(synsets, 1):\n",
    "        print(f\"{i}. {synset.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get only noun meanings of a word\n",
    "def get_noun_meanings(word):\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    print(f\"Noun definitions for '{word}':\")\n",
    "    for i, synset in enumerate(synsets, 1):\n",
    "        print(f\"{i}. {synset.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main program\n",
    "word = input(\"Enter a word: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions for 'Munich':\n",
      "1. the capital and largest city of Bavaria in southwestern Germany\n"
     ]
    }
   ],
   "source": [
    "# Get all meanings\n",
    "get_all_meanings(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun definitions for 'Munich':\n",
      "1. the capital and largest city of Bavaria in southwestern Germany\n"
     ]
    }
   ],
   "source": [
    "# Get noun meanings\n",
    "get_noun_meanings(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 of Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia-api\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a48c1d393458>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwikipediaapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import wikipediaapi\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wiki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fetch the Wikipedia article\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# wiki = wikipediaapi.Wikipedia(\"en\")\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mwiki\u001b[49m\u001b[38;5;241m.\u001b[39mpage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaharashtra\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wiki' is not defined"
     ]
    }
   ],
   "source": [
    "# Fetch the Wikipedia article\n",
    "wiki = wikipediaapi.Wikipedia(\"en\")\n",
    "page = wiki.page(\"Maharashtra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply Chunking and Show Chunks of Type <VBD><DT>\n",
    "doc = nlp(page.text)\n",
    "print(\"Chunks of type <VBD><DT>:\")\n",
    "for chunk in doc:\n",
    "    if chunk.pos_ == 'VERB' and chunk.dep_ == 'aux' and chunk.head.pos_ == 'DET':\n",
    "        print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find all the Named Entities in the Article\n",
    "print(\"\\nNamed Entities in the Article:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Access All the Verbs and Apply the Porter Stemmer and Lancaster Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "verbs = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "print(\"\\nVerbs with Porter and Lancaster Stemming:\")\n",
    "for verb in verbs:\n",
    "    print(f\"Original: {verb}, Porter Stemmed: {porter_stemmer.stem(verb)}, Lancaster Stemmed: {lancaster_stemmer.stem(verb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Access All the Verbs in Past Tense and Lemmatize Them. Show Only Unique Verbs\n",
    "past_tense_verbs = set([token.lemma_ for token in doc if token.pos_ == 'VERB' and token.tag_ == 'VBD'])\n",
    "print(\"\\nUnique Past Tense Verbs and their Lemmas:\")\n",
    "for verb in past_tense_verbs:\n",
    "    print(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Read Only the Adjectives and Find Their Synonyms\n",
    "adjectives = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "print(\"\\nSynonyms for Adjectives:\")\n",
    "for adj in adjectives:\n",
    "    synsets = wn.synsets(adj, pos=wn.ADJ)\n",
    "    if synsets:\n",
    "        print(f\"{adj}: {[lemma.name() for lemma in synsets[0].lemmas()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Read All the Verbs and Print Their Antonyms\n",
    "print(\"\\nAntonyms for Verbs:\")\n",
    "for verb in verbs:\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                print(f\"{verb} -> {[ant.name() for ant in lemma.antonyms()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Print All Entities Marked as DATE and TIME\n",
    "print(\"\\nEntities marked as DATE and TIME:\")\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in ['DATE', 'TIME']:\n",
    "        print(f\"{ent.text} ({ent.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Print All the Named Locations in the Article\n",
    "print(\"\\nNamed Locations in the Article:\")\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'GPE':  # GPE stands for Geo-Political Entity (locations)\n",
    "        print(ent.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
